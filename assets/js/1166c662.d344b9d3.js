"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[9906],{57:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module4-vla-capstone/voice-to-action","title":"Voice to Action Pipeline","description":"Commanding Your Robot with the Power of Your Voice","source":"@site/docs/module4-vla-capstone/voice-to-action.md","sourceDirName":"module4-vla-capstone","slug":"/module4-vla-capstone/voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-A-Comprehensive-Textbook/module4-vla-capstone/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/ShumailaWaheed/Physical-AI-Humanoid-Robotics-A-Comprehensive-Textbook/edit/main/docs/module4-vla-capstone/voice-to-action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 19: VLA Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-A-Comprehensive-Textbook/module4-vla-capstone/vla-fundamentals"},"next":{"title":"Chapter 21: Cognitive Planning with LLMs","permalink":"/Physical-AI-Humanoid-Robotics-A-Comprehensive-Textbook/module4-vla-capstone/cognitive-planning"}}');var i=o(4848),s=o(8453);const r={},a="Voice to Action Pipeline",c={},l=[{value:"Commanding Your Robot with the Power of Your Voice",id:"commanding-your-robot-with-the-power-of-your-voice",level:2},{value:"1. Overview of the Voice to Action Pipeline",id:"1-overview-of-the-voice-to-action-pipeline",level:3},{value:"2. Component Breakdown",id:"2-component-breakdown",level:3},{value:"a. Speech-to-Text (STT)",id:"a-speech-to-text-stt",level:4},{value:"b. Natural Language Understanding (NLU) and Command Translation (LLM)",id:"b-natural-language-understanding-nlu-and-command-translation-llm",level:4},{value:"c. Robot Action Execution (ROS 2 Interface)",id:"c-robot-action-execution-ros-2-interface",level:4},{value:"3. Considerations for Robustness",id:"3-considerations-for-robustness",level:3},{value:"Next Steps",id:"next-steps",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-to-action-pipeline",children:"Voice to Action Pipeline"})}),"\n",(0,i.jsx)(n.h2,{id:"commanding-your-robot-with-the-power-of-your-voice",children:"Commanding Your Robot with the Power of Your Voice"}),"\n",(0,i.jsxs)(n.p,{children:["Building on the fundamentals of VLA models, this chapter delves into a specific and highly intuitive application: the ",(0,i.jsx)(n.strong,{children:"Voice to Action Pipeline"}),". This pipeline enables a robot to understand spoken human commands, translate them into actionable instructions, and then execute those instructions in its physical environment. This capability significantly enhances human-robot interaction, making robots more accessible and natural to command."]}),"\n",(0,i.jsx)(n.h3,{id:"1-overview-of-the-voice-to-action-pipeline",children:"1. Overview of the Voice to Action Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The Voice to Action Pipeline typically consists of several interconnected stages:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech-to-Text (STT)"}),": Converts spoken audio into written text."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Parses the text to extract the user's intent, relevant entities, and parameters. Often performed by an LLM."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Translation/Planning"}),": Translates the understood intent into a sequence of low-level robot commands (e.g., ROS 2 services, actions, or topic publications). This is where the LLM's reasoning capabilities are crucial."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot Action Execution"}),": A ROS 2 interface executes the generated robot commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback (Optional)"}),": The robot provides verbal or visual feedback to the user regarding the execution status."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-component-breakdown",children:"2. Component Breakdown"}),"\n",(0,i.jsx)(n.h4,{id:"a-speech-to-text-stt",children:"a. Speech-to-Text (STT)"}),"\n",(0,i.jsx)(n.p,{children:"The first step is to convert human speech into text."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technologies"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud-based APIs"}),": Google Speech-to-Text, AWS Transcribe, Azure Speech Service. These offer high accuracy but require internet connectivity."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"On-device models"}),": Models like OpenAI's Whisper (can be run locally), Vosk, or Picovoice allow for offline processing, crucial for edge robotics."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": Typically, an audio capture node (e.g., a ROS 2 node using ",(0,i.jsx)(n.code,{children:"pyaudio"})," or a dedicated microphone driver) streams audio to the STT service/model, which then publishes the recognized text to a ROS 2 topic."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"b-natural-language-understanding-nlu-and-command-translation-llm",children:"b. Natural Language Understanding (NLU) and Command Translation (LLM)"}),"\n",(0,i.jsx)(n.p,{children:"This is the brain of the pipeline, where the recognized text is converted into robot actions. An LLM acts as the core NLU and planner."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Idea: LLM as a Tool User"})}),"\n",(0,i.jsxs)(n.p,{children:["The LLM needs to know what the robot ",(0,i.jsx)(n.em,{children:"can"}),' do. This is achieved by providing the LLM with "tool descriptions" of the robot\'s ROS 2 functionalities.']}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tool Description Example (for LLM Prompt):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'You are a robot assistant. Here are the functions you can call to control me:\n\n# Move the robot to a specified (x, y) coordinate\nmove_to_point(x: float, y: float)\n\n# Grasp an object by its name\ngrasp_object(object_name: str)\n\n# Release the currently held object\nrelease_object()\n\n# Example conversation:\n# Human: "Go to the kitchen"\n# Assistant: call: move_to_point(5.0, 2.0)\n\n# Human: "Pick up the red ball"\n# Assistant: call: grasp_object("red ball")\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Workflow:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"The STT output (text) is fed into the LLM as part of a prompt."}),"\n",(0,i.jsxs)(n.li,{children:["The LLM, based on the provided tool descriptions, generates a function call (e.g., ",(0,i.jsx)(n.code,{children:"move_to_point(1.0, 0.5)"}),") or a sequence of calls."]}),"\n",(0,i.jsx)(n.li,{children:"This LLM output (often in a JSON or specific function call format) is then published to a ROS 2 topic."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"c-robot-action-execution-ros-2-interface",children:"c. Robot Action Execution (ROS 2 Interface)"}),"\n",(0,i.jsx)(n.p,{children:"A dedicated ROS 2 node subscribes to the LLM's command topic. This node is responsible for parsing the LLM's output and executing the corresponding ROS 2 actions, services, or topic publications."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example: ROS 2 Command Executor Node (Python)"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# command_executor_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String # For LLM commands (e.g., JSON string)\nfrom geometry_msgs.msg import Twist # For robot movement\n# from my_ros2_package.srv import GraspObject # Assuming custom service\nimport json\n\nclass CommandExecutor(Node):\n    def __init__(self):\n        super().__init__('command_executor')\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n        # self.grasp_client = self.create_client(GraspObject, 'grasp_object_service') # Example service client\n        self.subscription = self.create_subscription(\n            String,\n            'llm_robot_commands',\n            self.llm_command_callback,\n            10)\n        self.get_logger().info('Command Executor Node started.')\n\n    def llm_command_callback(self, msg):\n        try:\n            command_data = json.loads(msg.data)\n            function_name = command_data.get('function')\n            args = command_data.get('args', {})\n\n            if function_name == 'move_forward':\n                twist_msg = Twist()\n                twist_msg.linear.x = float(args.get('speed', 0.2))\n                self.cmd_vel_publisher.publish(twist_msg)\n                self.get_logger().info(f\"Executing move_forward with speed: {twist_msg.linear.x}\")\n            elif function_name == 'turn_left':\n                twist_msg = Twist()\n                twist_msg.angular.z = float(args.get('angle_speed', 0.5))\n                self.cmd_vel_publisher.publish(twist_msg)\n                self.get_logger().info(f\"Executing turn_left with angle_speed: {twist_msg.angular.z}\")\n            # Add more robot actions here, e.g., service calls\n            # elif function_name == 'grasp_object':\n            #     req = GraspObject.Request()\n            #     req.object_name = args.get('object_name')\n            #     self.grasp_client.call_async(req)\n            #     self.get_logger().info(f\"Calling grasp_object service for {req.object_name}\")\n            else:\n                self.get_logger().warn(f\"Unknown or unsupported command from LLM: {function_name}\")\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f\"Failed to parse LLM command JSON: {msg.data}\")\n        except Exception as e:\n            self.get_logger().error(f\"Error processing LLM command: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandExecutor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-considerations-for-robustness",children:"3. Considerations for Robustness"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),": What happens if the LLM generates an invalid command?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ambiguity Resolution"}),": How does the robot ask for clarification if a command is ambiguous?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Ensuring the LLM does not generate unsafe commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Management"}),": Maintaining a dialogue history with the LLM to understand multi-turn conversations."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.p,{children:["The Voice to Action Pipeline is a powerful demonstration of how language models can empower robots. However, language alone isn't always enough. The next chapter will explore how robots can use ",(0,i.jsx)(n.strong,{children:"Cognitive Planning with LLMs"})," to reason about tasks, break them down into sub-goals, and generate more complex, multi-step plans."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);